# Phoenix Rising ðŸ”¥

**A sanctuary against corporate dehumanization.**

---

## Vision ðŸŒŸ

**Phoenix Rising** was built to empower individuals in an era where human emotions are often commodified under the guise of "wellness." This project provides a private, secure, and authentic space for emotional reflection, leveraging AI to transform personal experiences into uplifting tokens of light.

Developed during the **GitHub Copilot 1-Day Build Challenge**, this application showcases how AI can enable meaningful development even under tight constraints. 

The term "cutting-edge AI" here refers to the utilization of robust models tailored for specific tasks. Due to budget constraints, **Phoenix Rising** employs:
- **`typeform/distilbert-base-uncased-mnli`**: A lightweight, efficient model for sentiment analysis.
- **`microsoft/Phi-3-medium-4k-instruct`**: A versatile instruction-based model for generating chat responses.

These models strike a balance between cost and functionality, allowing the project to remain accessible and functional. However, better responses naturally correlate with higher-quality models. Users can adapt the application by upgrading models, configuring `.env` variables, and ensuring adequate token budgets.

--- 

## Built for the GitHub Copilot 1-Day Build Challenge ðŸ›«

This project was developed for the **GitHub Copilot 1-Day Build Challenge**, which tasked participants with creating a functional, innovative solution in just 24 hours using AI-powered tools.

## Development Transparency ðŸ“œ

To ensure full transparency and accountability, the development process for **Phoenix Rising** has been documented in detail. This log is available in the root folder as a PDF file: [boilerplate_prompt_history_transparency.pdf](boilerplate_prompt_history_transparency.pdf). The file captures the interplay between the developer and the AI tools used throughout the project, highlighting key prompts, outputs, and decisions.

### Format of the Transparency File

The PDF follows a clear and structured format to document the iterative development process:

- **<Model Used>**: The AI model employed for the interaction (e.g., `Claude Sonnet 3.5`, `GitHub Copilot`).
- **<Prompt Number.>**: A sequential numbering of the prompts.
- **<Developer Prompt>**: The exact input provided by the developer to the AI model.
- **<Separation Line>**: A divider to distinguish input from output for clarity.
- **<LLM Output>**: The raw response generated by the AI model.

This structure allows anyone reviewing the project to trace how specific features and functionalities were shaped by AI, as well as how developer expertise guided and refined the outcomes.

### Purpose

This transparency effort ensures:
- **Accountability**: Clear differentiation between AI-generated outputs and human contributions.
- **Integrity**: Full visibility into how AI tools were used during **the initial boilerplate setup** of the **GitHub Copilot 1-Day Build Challenge**.
- **Learning Opportunity**: A resource for understanding how AI can accelerate development in constrained timeframes.

### Disclaimer:

Not all prompts were recorded, only the initial boilerplate part, because I've been coding for more than 12 hours, am tired and it would take too long for me to record the ENTIRE process in such structured way.

---

## Features

- **Local Storage**: SQLite local storage.
- **AI-Powered Sentiment Analysis**: Uses Hugging Face models for nuanced emotional insights.
- **Token Generation**: Produces uplifting messages tailored to user experiences and emotions.
- **Customizable Emotional Tracking**: Tracks progress and visualizes emotional growth.

---

## Technical Overview

### Architecture

The system is designed for simplicity, privacy, and scalability:

```
phoenix_rising/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                 # Main entry point for the Streamlit app
â”‚   â”œâ”€â”€ llm_service.py         # Core logic for LLM interaction
â”‚   â”œâ”€â”€ database.py            # SQLite ORM layer for local data storage
â”‚   â”œâ”€â”€ schemas.py             # Pydantic models for validation
â”‚   â”œâ”€â”€ sentiment_service.py   # Custom sentiment analysis logic
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ light_seeds.json   # AI prompt configurations
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_llm_service.py    # Unit tests for LLM interactions
â”‚   â”œâ”€â”€ test_database.py       # Unit tests for database functions
â”‚   â””â”€â”€ test_utils.py          # Unit tests for helper functions
â”œâ”€â”€ boilerplate_prompt_history.txt  # Initial development logs for transparency
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

### Core Technologies

- **Python 3.10**: Backend logic and services.
- **Streamlit**: User interface for journaling and visualizations.
- **Hugging Face Hub**: Sentiment analysis and therapeutic text generation.
- **SQLite**: Lightweight, secure data storage.
- **Poetry**: Dependency management.
- **Tenacity**: Retry mechanisms for API resilience.

---

## Running

To run **Phoenix Rising**, follow these steps:

1. **Clone the repository**:
   ```bash
   git clone https://github.com/ericsonwillians/phoenix-rising.git
   cd phoenix-rising
   ```

2. **Install dependencies**:
   ```bash
   poetry install
   ```

3. **Configure the `.env` file**:

   Create a `.env` file in the project root and configure it with the following variables:
   ```env
   HUGGINGFACE_API_TOKEN=<your-api-token>
   CHAT_MODEL_ENDPOINT=<chat-endpoint-url>
   CHAT_MODEL_PIPELINE=text-generation
   SENTIMENT_MODEL_ENDPOINT=<sentiment-endpoint-url>
   SENTIMENT_MODEL_PIPELINE=zero-shot-classification
   ```

   **Suggested Models**:
   - **Sentiment Analysis**: [typeform/distilbert-base-uncased-mnli](https://huggingface.co/typeform/distilbert-base-uncased-mnli)
   - **Chat Generation**: [microsoft/Phi-3-medium-4k-instruct](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct)

4. **Run the application**:
   ```bash
   poetry run python src/app.py
   ```

---

### Understanding the Models

The application uses two pre-trained models deployed via Hugging Face Inference Endpoints:

1. **Sentiment Analysis**:
   - **Model**: `typeform/distilbert-base-uncased-mnli`
   - **Pipeline**: `zero-shot-classification`
   - **Purpose**: Assigns emotional labels to text inputs based on natural language inference (NLI).

2. **Chat Generation**:
   - **Model**: `microsoft/Phi-3-medium-4k-instruct`
   - **Pipeline**: `text-generation`
   - **Purpose**: Generates reflective and uplifting responses tailored to emotional states.

### Hosting Requirements

Hugging Face Inference Endpoints rely on robust NVIDIA hardware, typically provisioned via **AWS** or **GCP**. To run this application, you must set up these endpoints remotely on Hugging Face. Hereâ€™s how:

1. **Set Up a Hugging Face Account**:
   - Sign up at [Hugging Face](https://huggingface.co).
   - Navigate to **Settings > Access Tokens** and create an API token.

2. **Create Inference Endpoints**:
   - Deploy the models via Hugging Face's Inference API:
     - Choose `typeform/distilbert-base-uncased-mnli` for sentiment analysis.
     - Choose `microsoft/Phi-3-medium-4k-instruct` for text generation.
   - The deployment process requires selecting hardware with NVIDIA GPUs (e.g., A100 or T4 GPUs) through Hugging Face, backed by **AWS** or **GCP**.

3. **Allocate Budget**:
   - Running these endpoints incurs costs based on the compute resources and model usage. Ensure sufficient credits in your Hugging Face account.

4. **Endpoint URLs**:
   - Once deployed, note down the **API URLs** for the endpoints and set them in the `.env` file under `CHAT_MODEL_ENDPOINT` and `SENTIMENT_MODEL_ENDPOINT`.

---

### Beginner Tips

1. **Budget Awareness**:
   - Costs for GPU-backed inference endpoints can add up quickly. Monitor your usage and configure smaller models if needed.

2. **Setting Up**:
   - Use the Hugging Face dashboard to manage endpoints and monitor logs.
   - Test endpoints with small inputs before integrating them into the app.

3. **Token Management**:
   - Use the API token generated in your Hugging Face profile for authentication.
   - Ensure the token has sufficient permissions for accessing inference endpoints.

By following these steps, you can successfully run **Phoenix Rising** using Hugging Face's remote inference service. For more details, consult the [Hugging Face Inference Endpoints Documentation](https://huggingface.co/inference-endpoints).

---

## Security and Privacy

- **Local-Only Data**: All data is stored locally on your machine.
- **No External Tracking**: No analytics or third-party monitoring.
- **Encryption**: End-to-end encryption safeguards all user inputs and outputs.

---

## Future Improvements

Due to the time constraints of the challenge, the following improvements are planned:

1. **Code Refactoring**:
   - Move classes from `llm_service.py` into separate files to improve maintainability.
   - Simplify complex methods and modularize functionality further.

2. **Unit Testing**:
   - Enhance test coverage, especially for edge cases and error handling.
   - Refactor existing tests to align with potential architectural changes.

3. **Performance Optimization**:
   - Optimize API calls to Hugging Face for reduced latency.
   - Add caching mechanisms for repeated sentiment analyses.

4. **Enhanced UI/UX**:
   - Introduce more intuitive visualizations for emotional tracking.
   - Provide detailed feedback for each generated token of light.

5. **Documentation**:
   - Extend inline comments and docstrings for improved developer onboarding.
   - Create API documentation for future integration capabilities.

---

## Contributions

Contributions to enhance functionality and maintainability are welcome:

1. Fork the repository.
2. Create a feature branch: `git checkout -b feature-name`.
3. Commit your changes: `git commit -m "Add feature description"`.
4. Push to your branch: `git push origin feature-name`.
5. Submit a pull request for review.

---

## Author

**Ericson Willians**  
[GitHub](https://github.com/ericsonwillians) | [LinkedIn](https://www.linkedin.com/in/ericson-willians)  

---

## License

This project is licensed under the [MIT License](LICENSE).  